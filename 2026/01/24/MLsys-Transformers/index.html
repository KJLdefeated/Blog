<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="keywords" content="Hexo Theme Redefine">
    
    <meta name="author" content="Kai-Jie Lin">
    
    <!-- Completely eliminate flash of wrong theme -->
    <script>
        (function() {
            const THEME_KEY = "REDEFINE-THEME-STATUS";
            const DARK = "dark", LIGHT = "light";
            
            // Get preferred theme
            function getTheme() {
                try {
                    const saved = localStorage.getItem(THEME_KEY);
                    if (saved) {
                        const { isDark } = JSON.parse(saved);
                        return isDark ? DARK : LIGHT;
                    }
                } catch (e) {}
                
                return matchMedia("(prefers-color-scheme: dark)").matches ? DARK : LIGHT;
            }
            
            // Apply theme to document
            function applyTheme(theme) {
                const isDark = theme === DARK;
                const root = document.documentElement;
                
                // Set classes for compatibility
                root.classList.add(theme);
                root.classList.remove(isDark ? LIGHT : DARK);
                root.style.colorScheme = theme;
            }
            
            // Initial application
            const theme = getTheme();
            applyTheme(theme);
            
            // Listen for system preference changes
            matchMedia("(prefers-color-scheme: dark)").addEventListener("change", ({ matches }) => {
                // Only update if using system preference (no localStorage entry)
                if (!localStorage.getItem(THEME_KEY)) {
                    applyTheme(matches ? DARK : LIGHT);
                }
            });
            
            // Set body classes ASAP (before DOMContentLoaded)
            (function() {
                const addBodyClass = () => {
                    const b = document.body;
                    if (!b) return false;
                    b.classList.add(theme + "-mode");
                    return true;
                };

                // Try immediately
                if (addBodyClass()) return;

                // Observe until body exists
                const mo = new MutationObserver(() => {
                    if (addBodyClass()) mo.disconnect();
                });
                mo.observe(document.documentElement, { childList: true, subtree: true });
            })();
        })();
    </script>
    
    <!-- preconnect -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>

    
    <!--- Seo Part-->
    
    <link rel="canonical" href="https://kjldefeated.github.io/Blog/2026/01/24/mlsys-transformers/"/>
    <meta name="robots" content="index,follow">
    <meta name="googlebot" content="index,follow">
    <meta name="revisit-after" content="1 days">
    
    
    
        
        <meta name="description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:type" content="article">
<meta property="og:title" content="MLsys Note (0) - Qwen3 Code Walk Through">
<meta property="og:url" content="https://kjldefeated.github.io/Blog/2026/01/24/MLsys-Transformers/">
<meta property="og:site_name" content="KJ&#39;s Blog">
<meta property="og:description" content="Hexo Theme Redefine, Redefine Your Hexo Journey.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://kjldefeated.github.io/images/redefine-og.webp">
<meta property="article:published_time" content="2026-01-24T10:03:24.000Z">
<meta property="article:modified_time" content="2026-01-29T04:03:18.349Z">
<meta property="article:author" content="KJL">
<meta property="article:tag" content="MLsys">
<meta property="article:tag" content="Research">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://kjldefeated.github.io/images/redefine-og.webp">
    
    
        <!-- Google tag (gtag.js) -->
        <script src="https://www.googletagmanager.com/gtag/js?id=G-2728M4QDJ7"></script>
        <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());

        gtag('config', 'G-2728M4QDJ7');
        </script>
    
    <!--- Icon Part-->
    <link rel="icon" type="image/png" href="/Blog/images/favicon.ico" sizes="192x192">
    <link rel="apple-touch-icon" sizes="180x180" href="/Blog/images/favicon.ico">
    <meta name="theme-color" content="#A31F34">
    <link rel="shortcut icon" href="/Blog/images/favicon.ico">
    <!--- Page Info-->
    
    <title>
        
            MLsys Note (0) - Qwen3 Code Walk Through | KJ&#39;s Blog
        
    </title>

    
<link rel="stylesheet" href="/Blog/fonts/Chillax/chillax.css">


    <!--- Inject Part-->
    

    
<link rel="stylesheet" href="/Blog/css/style.css">


    
        
<link rel="stylesheet" href="/Blog/css/build/tailwind.css">

    

    
<link rel="stylesheet" href="/Blog/fonts/GeistMono/geist-mono.css">

    
<link rel="stylesheet" href="/Blog/fonts/Geist/geist.css">

    <!--- Font Part-->
    
    
    
    
    
    

    <script id="hexo-configurations">
    window.config = {"hostname":"kjldefeated.github.io","root":"/Blog/","language":"en"};
    window.theme = {"articles":{"style":{"font_size":"16px","line_height":1.5,"image_border_radius":"14px","image_alignment":"center","image_caption":false,"link_icon":true,"delete_mask":false,"title_alignment":"left","headings_top_spacing":{"h1":"3.2rem","h2":"2.4rem","h3":"1.9rem","h4":"1.6rem","h5":"1.4rem","h6":"1.3rem"}},"word_count":{"enable":true,"count":true,"min2read":true},"author_label":{"enable":true,"auto":false,"list":[]},"code_block":{"copy":true,"style":"mac","highlight_theme":{"light":"github","dark":"vs2015"},"font":{"enable":false,"family":null,"url":null}},"toc":{"enable":true,"max_depth":3,"number":false,"expand":true,"init_open":true},"copyright":{"enable":false,"default":"cc_by_nc_sa"},"lazyload":true,"pangu_js":false,"recommendation":{"enable":false,"title":"推荐阅读","limit":3,"mobile_limit":2,"placeholder":"/images/haykyuu.jpg","skip_dirs":[]}},"colors":{"primary":"#A31F34","secondary":null,"default_mode":"dark"},"global":{"fonts":{"chinese":{"enable":false,"family":null,"url":null},"english":{"enable":false,"family":null,"url":null},"title":{"enable":false,"family":null,"url":null}},"content_max_width":"1000px","sidebar_width":"210px","hover":{"shadow":true,"scale":false},"scroll_progress":{"bar":false,"percentage":true},"website_counter":{"url":"https://cn.vercount.one/js","enable":true,"site_pv":true,"site_uv":true,"post_pv":true},"single_page":true,"preloader":{"enable":false,"custom_message":null},"side_tools":{"gear_rotation":true,"auto_expand":false},"open_graph":{"enable":true,"image":"/images/redefine-og.webp","description":"Hexo Theme Redefine, Redefine Your Hexo Journey."},"google_analytics":{"enable":true,"id":"G-2728M4QDJ7"}},"home_banner":{"enable":true,"style":"fixed","image":{"light":"/images/haykyuu.jpg","dark":"/images/haykyuu.jpg"},"title":"KJ's Blog","subtitle":{"text":[],"hitokoto":{"enable":false,"show_author":false,"api":"https://v1.hitokoto.cn"},"typing_speed":100,"backing_speed":80,"starting_delay":500,"backing_delay":1500,"loop":true,"smart_backspace":true},"text_color":{"light":"#fff","dark":"#fff"},"text_style":{"title_size":"2.8rem","subtitle_size":"1.5rem","line_height":1.2},"custom_font":{"enable":false,"family":null,"url":null},"social_links":{"enable":true,"style":"default","links":{"github":"https://github.com/KJLdefeated","instagram":null,"zhihu":null,"twitter":"https://x.com/Kjl0508Sc10","email":"linkai0508@gmail.com"},"qrs":{"weixin":null}}},"plugins":{"feed":{"enable":false},"aplayer":{"enable":false,"type":"fixed","audios":[{"name":null,"artist":null,"url":null,"cover":null,"lrc":null}]},"mermaid":{"enable":false,"version":"11.4.1"}},"version":"2.8.5","navbar":{"auto_hide":false,"color":{"left":"#f78736","right":"#367df7","transparency":35},"width":{"home":"1200px","pages":"1000px"},"links":{"Home":{"path":"/","icon":"fa-regular fa-house"}},"search":{"enable":false,"preload":true}},"page_templates":{"friends_column":2,"tags_style":"blur"},"home":{"sidebar":{"enable":true,"position":"left","first_item":"menu","announcement":null,"show_on_mobile":true,"links":null},"article_date_format":"auto","excerpt_length":200,"categories":{"enable":true,"limit":3},"tags":{"enable":true,"limit":3}},"footerStart":"2022/8/17 11:45:14"};
    window.lang_ago = {"second":"%s seconds ago","minute":"%s minutes ago","hour":"%s hours ago","day":"%s days ago","week":"%s weeks ago","month":"%s months ago","year":"%s years ago"};
    window.data = {"masonry":false};
  </script>
    
    <!--- Fontawesome Part-->
    
<link rel="stylesheet" href="/Blog/fontawesome/fontawesome.min.css">

    
<link rel="stylesheet" href="/Blog/fontawesome/brands.min.css">

    
<link rel="stylesheet" href="/Blog/fontawesome/solid.min.css">

    
<link rel="stylesheet" href="/Blog/fontawesome/regular.min.css">

    
    
    
    
<!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" integrity="sha512-fHwaWebuwA7NSF5Qg/af4UeDx9XqUpYpOGgubo3yWu+b2IQR4UeQwbb42Ti7gVAjNtVoI/I9TEoYeu9omwcC6g==" crossorigin="anonymous" referrerpolicy="no-referrer" /><!-- hexo injector head_end end --><meta name="generator" content="Hexo 8.1.1"></head>



<body>
	<div class="progress-bar-container">
	

	
	<span class="pjax-progress-bar"></span>
	<!--        <span class="swup-progress-icon">-->
	<!--            <i class="fa-solid fa-circle-notch fa-spin"></i>-->
	<!--        </span>-->
	
</div>

<main class="page-container" id="swup">

	

	<div class="main-content-container flex flex-col justify-between min-h-dvh">
		<div class="main-content-header">
			<header class="navbar-container px-6 md:px-12">
    <div class="navbar-content transition-navbar ">
        <div class="left">
            
                <a class="logo-image h-8 w-8 sm:w-10 sm:h-10 mr-3" href="/Blog/">
                    <img src="/Blog/images/favicon.ico" class="w-full h-full rounded-xs">
                </a>
            
            <a class="logo-title" href="/Blog/">
                
                KJ&#39;s Blog
                
            </a>
        </div>

        <div class="right">
            <!-- PC -->
            <div class="desktop">
                <ul class="navbar-list">
                    
                        
                            

                            <li class="navbar-item">
                                <!-- Menu -->
                                <a class=""
                                   href="/Blog/"
                                        >
                                    <i class="fa-regular fa-house fa-fw"></i>
                                    HOME
                                    
                                </a>

                                <!-- Submenu -->
                                
                            </li>
                    
                    
                </ul>
            </div>
            <!-- Mobile -->
            <div class="mobile">
                
                <div class="icon-item navbar-bar">
                    <div class="navbar-bar-middle"></div>
                </div>
            </div>
        </div>
    </div>

    <!-- Mobile sheet -->
    <div class="navbar-drawer h-dvh w-full absolute top-0 left-0 bg-background-color flex flex-col justify-between">
        <ul class="drawer-navbar-list flex flex-col px-4 justify-center items-start">
            
                
                    

                    <li class="drawer-navbar-item text-base my-1.5 flex flex-col w-full">
                        
                        <a class="py-1.5 px-2 flex flex-row items-center justify-between gap-1 hover:!text-primary active:!text-primary text-2xl font-semibold group border-b border-border-color hover:border-primary w-full "
                           href="/Blog/"
                        >
                            <span>
                                HOME
                            </span>
                            
                                <i class="fa-regular fa-house fa-sm fa-fw"></i>
                            
                        </a>
                        

                        
                    </li>
            

            
            
        </ul>

        <div class="statistics flex justify-around my-2.5">
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/Blog/tags">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">8</div>
        <div class="label text-third-text-color text-sm">Tags</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/Blog/categories">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">0</div>
        <div class="label text-third-text-color text-sm">Categories</div>
    </a>
    <a class="item tag-count-item flex flex-col justify-center items-center w-20" href="/Blog/archives">
        <div class="number text-2xl sm:text-xl text-second-text-color font-semibold">24</div>
        <div class="label text-third-text-color text-sm">Posts</div>
    </a>
</div>
    </div>

    <div class="window-mask"></div>

</header>


		</div>

		<div class="main-content-body transition-fade-up">
			

			<div class="main-content">
				<div class="post-page-container flex relative justify-between box-border w-full h-full">
	<div class="article-content-container">

		<div class="article-title relative w-full">
			
			
			
			<img src="https://i.pinimg.com/1200x/7b/1a/5f/7b1a5f278e6210e62e986ddcc1c3bbf1.jpg" alt="MLsys Note (0) - Qwen3 Code Walk Through" class="w-full h-60 sm:h-72 md:h-80 object-cover sm:rounded-t-large dark:brightness-75" />
			
			<div class="w-full flex items-center absolute bottom-0 justify-start">
				<h1 class="article-title-cover text-center mx-6 my-6 text-second-text-color bg-background-color-transparent px-4 py-3 text-3xl sm:text-4xl md:text-5xl font-semibold backdrop-blur-lg rounded-xl border border-border-color ">MLsys Note (0) - Qwen3 Code Walk Through</h1>
			</div>
			
		</div>

		
		<div class="article-header flex flex-row gap-2 items-center px-2 sm:px-6 md:px-8">
			<div class="avatar w-[46px] h-[46px] flex-shrink-0 rounded-medium border border-border-color p-[1px]">
				<img src="/Blog/images/saki.JPG">
			</div>
			<div class="info flex flex-col justify-between">
				<div class="author flex items-center">
					<span class="name text-default-text-color text-lg font-semibold">Kai-Jie Lin</span>
					
					<span class="author-label ml-1.5 text-xs px-2 py-0.5 rounded-small text-third-text-color border border-shadow-color-1">Lv3</span>
					
				</div>
				<div class="meta-info">
					<div class="article-meta-info">
    <span class="article-date article-meta-item">
        <i class="fa-regular fa-pen-fancy"></i>&nbsp;
        <span class="desktop">2026-01-24 18:03:24</span>
        <span class="mobile">2026-01-24 18:03:24</span>
        <span class="hover-info">Created</span>
    </span>
    
        <span class="article-date article-meta-item">
            <i class="fa-regular fa-wrench"></i>&nbsp;
            <span class="desktop">2026-01-29 12:03:18</span>
            <span class="mobile">2026-01-29 12:03:18</span>
            <span class="hover-info">Updated</span>
        </span>
    

    
    
        <span class="article-tags article-meta-item">
            <i class="fa-regular fa-tags"></i>&nbsp;
            <ul>
                
                    <li>
                        <a href="/Blog/tags/Research/">Research</a>&nbsp;
                    </li>
                
                    <li>
                        | <a href="/Blog/tags/MLsys/">MLsys</a>&nbsp;
                    </li>
                
            </ul>
        </span>
    

    
    
    
    
        <span class="article-pv article-meta-item">
            <i class="fa-regular fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"></span>
        </span>
    
</div>

				</div>
			</div>
		</div>
		

		


		<div class="article-content markdown-body px-2 sm:px-6 md:px-8 pb-8">
			<p>最近想要深入學習一些MLsys的開源框架，像Sglang, vLLM和Slime之類的，光閱讀code和文章記憶不深，想說自己來寫一下筆記。第一篇先以最簡單的huggingface transformers為主，順便複習一下以前沒注意到的東西。</p>
<h1 id="Transformers-Code-Walk-Through"><a href="#Transformers-Code-Walk-Through" class="headerlink" title="Transformers Code Walk Through"></a>Transformers Code Walk Through</h1><p>平常都在用Transformers做事，但裡面的架構卻不是很熟悉，想用看code的方式，去解析一下不同開源LLM的實作細節，這邊以<a class="link"   target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen3/modeling_qwen3.py" >Qwen3<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>和<a class="link"   target="_blank" rel="noopener" href="https://github.com/huggingface/transformers/blob/main/src/transformers/models/qwen2/modeling_qwen2.py" >Qwen2<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>為主，參雜一些個人理解和對其他模型的比較，不會到超級仔細，若是說錯了，歡迎指正。<br>這邊主要以huggingface的實作為主。</p>
<h3 id="CausalLM"><a href="#CausalLM" class="headerlink" title="CausalLM"></a>CausalLM</h3><p>“Causal” 就是因果關係的意思，CausalLM就是泛指目前最常見從左到右的LLM，最新的token看得到過去的token，但過去的token看不到未來的token。</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Qwen3ForCausalLM</span>(Qwen3PreTrainedModel, GenerationMixin)<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">LlamaForCausalLM</span>(LlamaPreTrainedModel, GenerationMixin)<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">DeepseekV3ForCausalLM</span>(DeepseekV3PreTrainedModel, GenerationMixin)<br></code></pre></td></tr></table></figure></div>
<p>這是第一層interface，還沒有真的進入model，主要是為了LLM生成logits之後，可以做採樣上的控制（GenerationMixin），若是有loss function的話也會在這裡計算loss。<br>因為這邊forward function很單純，而且大同小異，就直接貼上來：</p>
<details>

<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        input_ids: torch.LongTensor | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        attention_mask: torch.Tensor | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        position_ids: torch.LongTensor | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        past_key_values: Cache | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        inputs_embeds: torch.FloatTensor | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        labels: torch.LongTensor | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        use_cache: <span class="hljs-built_in">bool</span> | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        cache_position: torch.LongTensor | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">        logits_to_keep: <span class="hljs-built_in">int</span> | torch.Tensor = <span class="hljs-number">0</span>,</span><br><span class="hljs-params">        **kwargs: Unpack[TransformersKwargs],</span><br><span class="hljs-params">    </span>) -&gt; CausalLMOutputWithPast:<br>        outputs: BaseModelOutputWithPast = <span class="hljs-variable language_">self</span>.model(<br>            input_ids=input_ids,<br>            attention_mask=attention_mask,<br>            position_ids=position_ids,<br>            past_key_values=past_key_values,<br>            inputs_embeds=inputs_embeds,<br>            use_cache=use_cache,<br>            cache_position=cache_position,<br>            **kwargs,<br>        )<br><br>        hidden_states = outputs.last_hidden_state<br>        logits = <span class="hljs-variable language_">self</span>.lm_head(hidden_states)<br><br>        loss = <span class="hljs-literal">None</span><br>        <span class="hljs-keyword">if</span> labels <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            loss = <span class="hljs-variable language_">self</span>.loss_function(logits=logits, labels=labels, vocab_size=<span class="hljs-variable language_">self</span>.config.vocab_size, **kwargs)<br><br>        <span class="hljs-keyword">return</span> CausalLMOutputWithPast(<br>            loss=loss,<br>            logits=logits,<br>            past_key_values=outputs.past_key_values,<br>            hidden_states=outputs.hidden_states,<br>            attentions=outputs.attentions,<br>        )<br></code></pre></td></tr></table></figure></div>

<p>lm_head 是一個 hidden_size X vocab_size的linear layer，從hidden states生成logits</p>
</details>


<h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>主要架構層，這裡會把input text從頭到尾生成last hidden state。</p>
<p><strong>embed_tokens</strong>: Token embedding，從input_ids (1, 2, 3…) mapping 成hidden vector。</p>
<p><strong>layers</strong>: config.num_hidden_layers定義了LLM需要幾層layers，layers越多model就越大。</p>
<p><strong>norm</strong>: Qwen 2, 3 使用了 RMSnorm 取代傳統 layer norm，後面細講。</p>
<p><strong>rotary_emb</strong>: Rotary Position Embedding. 這邊等到後面細講。</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Qwen3Model</span>(<span class="hljs-title class_ inherited__">Qwen3PreTrainedModel</span>):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: Qwen3Config</span>):<br>        <span class="hljs-built_in">super</span>().__init__(config)<br>        <span class="hljs-variable language_">self</span>.padding_idx = config.pad_token_id<br>        <span class="hljs-variable language_">self</span>.vocab_size = config.vocab_size<br><br>        <span class="hljs-variable language_">self</span>.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, <span class="hljs-variable language_">self</span>.padding_idx)<br>        <span class="hljs-variable language_">self</span>.layers = nn.ModuleList(<br>            [Qwen3DecoderLayer(config, layer_idx) <span class="hljs-keyword">for</span> layer_idx <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(config.num_hidden_layers)]<br>        )<br>        <span class="hljs-variable language_">self</span>.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)<br>        <span class="hljs-variable language_">self</span>.rotary_emb = Qwen3RotaryEmbedding(config=config)<br>        <span class="hljs-variable language_">self</span>.gradient_checkpointing = <span class="hljs-literal">False</span><br>        <span class="hljs-variable language_">self</span>.has_sliding_layers = <span class="hljs-string">&quot;sliding_attention&quot;</span> <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.config.layer_types<br><br>        <span class="hljs-comment"># Initialize weights and apply final processing</span><br>        <span class="hljs-variable language_">self</span>.post_init()<br></code></pre></td></tr></table></figure></div>

<p>Forward function接受自定義input_embeds, attention_maks, position_ids和KV cache，可以有flexible的masking和forward的操作。</p>
<details>

<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params"></span><br><span class="hljs-params">    self,</span><br><span class="hljs-params">    input_ids: torch.LongTensor | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    attention_mask: torch.Tensor | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    position_ids: torch.LongTensor | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    past_key_values: Cache | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    inputs_embeds: torch.FloatTensor | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    use_cache: <span class="hljs-built_in">bool</span> | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    cache_position: torch.LongTensor | <span class="hljs-literal">None</span> = <span class="hljs-literal">None</span>,</span><br><span class="hljs-params">    **kwargs: Unpack[TransformersKwargs],</span><br><span class="hljs-params"></span>) -&gt; BaseModelOutputWithPast<br><br>    <span class="hljs-comment"># 1. Embedding forward</span><br>    <span class="hljs-keyword">if</span> inputs_embeds <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        inputs_embeds = <span class="hljs-variable language_">self</span>.embed_tokens(input_ids)<br><br>    <span class="hljs-comment"># 2. 取出過去的key value</span><br>    <span class="hljs-comment"># 關於KV Cache 之後用另一篇討論</span><br>    <span class="hljs-keyword">if</span> use_cache <span class="hljs-keyword">and</span> past_key_values <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        past_key_values = DynamicCache(config=<span class="hljs-variable language_">self</span>.config)<br><br>    <span class="hljs-comment"># KV Cache的position</span><br>    <span class="hljs-keyword">if</span> cache_position <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        past_seen_tokens = past_key_values.get_seq_length() <span class="hljs-keyword">if</span> past_key_values <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0</span><br>        cache_position = torch.arange(<br>            past_seen_tokens, past_seen_tokens + inputs_embeds.shape[<span class="hljs-number">1</span>], device=inputs_embeds.device<br>        )<br><br>    <span class="hljs-keyword">if</span> position_ids <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>        position_ids = cache_position.unsqueeze(<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># 3. masking的部分拿llama的代替比較簡潔</span><br>    <span class="hljs-comment"># 我們input的attention_mask是1D的，若要造出2D的attention mask，可以用這個function</span><br>    causal_mask = create_causal_mask(<br>            config=<span class="hljs-variable language_">self</span>.config,<br>            input_embeds=inputs_embeds,<br>            attention_mask=attention_mask,<br>            cache_position=cache_position,<br>            past_key_values=past_key_values,<br>            position_ids=position_ids,<br>        )<br><br>    hidden_states = inputs_embeds<br>    <span class="hljs-comment"># 4. Position embedding</span><br>    position_embeddings = <span class="hljs-variable language_">self</span>.rotary_emb(hidden_states, position_ids)<br><br>    <span class="hljs-comment"># 5. Decoder forwards</span><br>    <span class="hljs-keyword">for</span> decoder_layer <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.layers[: <span class="hljs-variable language_">self</span>.config.num_hidden_layers]:<br>        hidden_states = decoder_layer(<br>            hidden_states,<br>            attention_mask=causal_mask_mapping[decoder_layer.attention_type],<br>            position_embeddings=position_embeddings,<br>            position_ids=position_ids,<br>            past_key_values=past_key_values,<br>            use_cache=use_cache,<br>            cache_position=cache_position,<br>            **kwargs,<br>        )<br><br>    <span class="hljs-comment"># 6. Last hidden states.</span><br>    hidden_states = <span class="hljs-variable language_">self</span>.norm(hidden_states)<br>    <span class="hljs-keyword">return</span> BaseModelOutputWithPast(<br>        last_hidden_state=hidden_states,<br>        past_key_values=past_key_values <span class="hljs-keyword">if</span> use_cache <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span>,<br>    )<br></code></pre></td></tr></table></figure></div>
</details>

<h2 id="Decoder-layers"><a href="#Decoder-layers" class="headerlink" title="Decoder layers"></a>Decoder layers</h2><p>這邊會分開來講 [layer norm -&gt; attention -&gt; mlp]。 </p>
<h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>這邊就不細講 Attention 怎麼算的，之後看 FlashInfer 再一起看 Flash Attn 等計算和代碼。</p>
<p>看一些一般不會注意到的實作細節。<br>Code:</p>
<details>

<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Qwen3Attention</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: Qwen3Config, layer_idx: <span class="hljs-built_in">int</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.config = config<br>        <span class="hljs-variable language_">self</span>.layer_idx = layer_idx<br>        <span class="hljs-variable language_">self</span>.head_dim = <span class="hljs-built_in">getattr</span>(config, <span class="hljs-string">&quot;head_dim&quot;</span>, config.hidden_size // config.num_attention_heads)<br>        <span class="hljs-variable language_">self</span>.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads <span class="hljs-comment"># MHA &amp; GQA</span><br>        <span class="hljs-variable language_">self</span>.scaling = <span class="hljs-variable language_">self</span>.head_dim**-<span class="hljs-number">0.5</span><br>        <span class="hljs-variable language_">self</span>.attention_dropout = config.attention_dropout<br>        <span class="hljs-variable language_">self</span>.is_causal = <span class="hljs-literal">True</span><br><br>        <span class="hljs-variable language_">self</span>.q_proj = nn.Linear(<br>            config.hidden_size, <br>            config.num_attention_heads * <span class="hljs-variable language_">self</span>.head_dim, <br>            bias=config.attention_bias<br>        )<br>        <span class="hljs-variable language_">self</span>.k_proj = nn.Linear(<br>            config.hidden_size, <br>            config.num_key_value_heads * <span class="hljs-variable language_">self</span>.head_dim, <br>            bias=config.attention_bias<br>        )<br>        <span class="hljs-variable language_">self</span>.v_proj = nn.Linear(<br>            config.hidden_size, <br>            config.num_key_value_heads * <span class="hljs-variable language_">self</span>.head_dim, <br>            bias=config.attention_bias<br>        )<br>        <span class="hljs-variable language_">self</span>.o_proj = nn.Linear(<br>            config.num_attention_heads * <span class="hljs-variable language_">self</span>.head_dim, <br>            config.hidden_size, <br>            bias=config.attention_bias<br>        )<br>        <span class="hljs-variable language_">self</span>.q_norm = Qwen3RMSNorm(<span class="hljs-variable language_">self</span>.head_dim, eps=config.rms_norm_eps)  <span class="hljs-comment"># unlike olmo, only on the head dim!</span><br>        <span class="hljs-variable language_">self</span>.k_norm = Qwen3RMSNorm(<span class="hljs-variable language_">self</span>.head_dim, eps=config.rms_norm_eps)  <span class="hljs-comment"># thus post q_norm does not need reshape</span><br>        <span class="hljs-variable language_">self</span>.sliding_window = config.sliding_window <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.layer_type == <span class="hljs-string">&quot;sliding_attention&quot;</span> <span class="hljs-keyword">else</span> <span class="hljs-literal">None</span><br></code></pre></td></tr></table></figure></div>
</details>

<ol>
<li><p>注意Qwen3的attention bias是可以被關閉的，而Qwen2是強制開啟。</p>
<ul>
<li>Attn(Q,K,V)&#x3D;softmax(QK&#x2F;sqrt(d)​+<strong>bias</strong>)V</li>
<li>bias會讓llm有更強的instruction following，能夠更服從prompt，相對的關掉bias可以讓llm有更多exploration，這對reasoning model特別有用，因為CoT需要更廣的探索去獲取reasoning pattern。</li>
</ul>
</li>
<li><p>QK norm是Qwen3新加的，在Qwen2和llama等模型都沒有。</p>
<ul>
<li>原本attention score: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mtext>​</mtext><mo>=</mo><mfrac><mrow><msub><mi>q</mi><mi>i</mi></msub><mtext>⊤​</mtext><msub><mi>k</mi><mi>j</mi></msub></mrow><msqrt><mi>d</mi></msqrt></mfrac></mrow><annotation encoding="application/x-tex">s_{ij}​=\frac{q_{i}⊤​k_{j}}{\sqrt{d}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7167em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">ij</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mord">​</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.5314em;vertical-align:-0.538em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9934em;"><span style="top:-2.5335em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord sqrt mtight"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9378em;"><span class="svg-align" style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mtight" style="padding-left:0.833em;"><span class="mord mathnormal mtight">d</span></span></span><span style="top:-2.8978em;"><span class="pstrut" style="height:3em;"></span><span class="hide-tail mtight" style="min-width:0.853em;height:1.08em;"><svg xmlns="http://www.w3.org/2000/svg" width="400em" height="1.08em" viewBox="0 0 400000 1080" preserveAspectRatio="xMinYMin slice"><path d="M95,702
c-2.7,0,-7.17,-2.7,-13.5,-8c-5.8,-5.3,-9.5,-10,-9.5,-14
c0,-2,0.3,-3.3,1,-4c1.3,-2.7,23.83,-20.7,67.5,-54
c44.2,-33.3,65.8,-50.3,66.5,-51c1.3,-1.3,3,-2,5,-2c4.7,0,8.7,3.3,12,10
s173,378,173,378c0.7,0,35.3,-71,104,-213c68.7,-142,137.5,-285,206.5,-429
c69,-144,104.5,-217.7,106.5,-221
l0 -0
c5.3,-9.3,12,-14,20,-14
H400000v40H845.2724
s-225.272,467,-225.272,467s-235,486,-235,486c-2.7,4.7,-9,7,-19,7
c-6,0,-10,-1,-12,-3s-194,-422,-194,-422s-65,47,-65,47z
M834 80h400000v40h-400000z"/></svg></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.1022em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.5073em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">q</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0359em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span><span class="mord mtight">⊤​</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3281em;"><span style="top:-2.357em;margin-left:-0.0315em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2819em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.538em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>.</li>
<li>在沒有bias的之後，模型更新會直接由QK決定，若是訓練過於朝向某個token，QK值會變得很大，在這裏的RMSnorm是為了訓練穩定。</li>
<li>除了防止訓練爆掉，還有一個是為了不讓RL訓練overfit到某些token，在RL訓練中，某些token可能導致獲得高reward，這屬於某種reward hacking，這邊的normalize是為了RL訓練的穩定。</li>
<li>為什麼只對QK做norm? 注意這邊只對head dim做norm，意思就是說為了不消除不同head之間的訊息量，所以才只對head內做norm，若是V也做同樣的事，則會失去不同head之間的語意訊息。</li>
</ul>
</li>
<li><p>​Group Query Attention &#x2F; Multi-head Attention<br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/Blog/images/GQA.png"
                     
                ></p>
<ul>
<li>在Attention裡每個head代表學習不同的語意和邏輯，就是不同的sub-space，當然也可以被共享。</li>
<li>就像上圖一樣，最naive的MHA每個QKV都是獨立的，也消耗最多的算力（更多的KV weight和KV Cache存取）。</li>
<li>MQA是另一極端，所有V享有一個head，最有效率的同時也犧牲表現力，只在一些專門解small task的模型中使用。</li>
<li>GQA是兼顧效率和表現，享有較少算力的同時兼顧表現，現代模型大多使用這種架構。</li>
</ul>
</li>
</ol>
<h3 id="Rotary-Positional-Embedding-RoPE"><a href="#Rotary-Positional-Embedding-RoPE" class="headerlink" title="Rotary Positional Embedding (RoPE)"></a>Rotary Positional Embedding (RoPE)</h3><p>比較一下Absolute PE和Relative PE</p>
<ol>
<li>Absolute: x_t → x_t + pos_embedding[t]<ul>
<li>沒有相對位置的資訊</li>
</ul>
</li>
<li>Relative: QK^T + bias(i - j)<ul>
<li>bias需要另外訓練</li>
</ul>
</li>
</ol>
<p><strong>RoPE</strong>概念是利用旋轉角度給予位置資訊，RoPE 對每個 position t，把 QK 在 2D 子空間裡旋轉一個角度θt。賦予相對位置資訊的同時，也不用另外學習。當然，當 context length 拉得超級長的時候，短距離位置之間的資訊量會被壓平，這時候會用 yarn 之類的變體來處理（Deepseek）。</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Qwen3RotaryEmbedding</span>(nn.Module):<br>    inv_freq: torch.Tensor  <span class="hljs-comment"># fix linting for `register_buffer`</span><br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config: Qwen3Config, device=<span class="hljs-literal">None</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.max_seq_len_cached = config.max_position_embeddings<br>        <span class="hljs-variable language_">self</span>.original_max_seq_len = config.max_position_embeddings<br><br>        <span class="hljs-variable language_">self</span>.config = config<br><br>        <span class="hljs-variable language_">self</span>.rope_type = <span class="hljs-variable language_">self</span>.config.rope_parameters[<span class="hljs-string">&quot;rope_type&quot;</span>]<br>        rope_init_fn: <span class="hljs-type">Callable</span> = <span class="hljs-variable language_">self</span>.compute_default_rope_parameters<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.rope_type != <span class="hljs-string">&quot;default&quot;</span>:<br>            rope_init_fn = ROPE_INIT_FUNCTIONS[<span class="hljs-variable language_">self</span>.rope_type]<br>        inv_freq, <span class="hljs-variable language_">self</span>.attention_scaling = rope_init_fn(<span class="hljs-variable language_">self</span>.config, device)<br><br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;inv_freq&quot;</span>, inv_freq, persistent=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.register_buffer(<span class="hljs-string">&quot;original_inv_freq&quot;</span>, inv_freq.clone(), persistent=<span class="hljs-literal">False</span>)<br></code></pre></td></tr></table></figure></div>
<p>inv_freq: 決定每一個維度的旋轉頻率。這邊 register_buffer 之後就是一個固定參數，不會被train。</br><br>rope_type: 這邊定義了不同種 rope，像是有處理過長 context 的 yarn。</p>
<p>計算 <code>inv_freq</code> （簡化）：對於第i個位置，分配一個角度。</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_default_rope_parameters</span>(<span class="hljs-params">config, device, seq_len=<span class="hljs-literal">None</span></span>):<br>    base = config.rope_parameters[<span class="hljs-string">&quot;rope_theta&quot;</span>] <span class="hljs-comment"># 通常是10000</span><br>    dim = config.head_dim<br>    inv_freq = <span class="hljs-number">1.0</span> / (<br>        base ** (torch.arange(<span class="hljs-number">0</span>, dim, <span class="hljs-number">2</span>) / dim)<br>    )<br></code></pre></td></tr></table></figure></div>

<p>Forward function: (簡化)</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x, position_ids</span>):<br>    freqs = (inv_freq @ position_ids).transpose(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>)<br>    emb = torch.cat((freqs, freqs), dim=-<span class="hljs-number">1</span>)<br>    cos = emb.cos()<br>    sin = emb.sin()<br>    <span class="hljs-keyword">return</span> cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)<br></code></pre></td></tr></table></figure></div>
<p>這邊比較簡單，就是對每個position_ids賦予cos和sin作為positional embedding。</p>
<h3 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h3><p>每個layer後面都會接一個MLP，之前一直不知道為什麼後面接一個這個，稍微研究一下才知道是為了引入「非線性」。</br><br>就像linear layer後面會有ReLU一樣，Decoder block最後也會有MLP。</br><br><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="/Blog/images/MLP.png"
                     
                ><br>Qwen2,3都採用<strong>Gated MLP</strong>。</p>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Qwen3MLP</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, config</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.config = config<br>        <span class="hljs-variable language_">self</span>.hidden_size = config.hidden_size<br>        <span class="hljs-variable language_">self</span>.intermediate_size = config.intermediate_size<br>        <span class="hljs-variable language_">self</span>.gate_proj = nn.Linear(<span class="hljs-variable language_">self</span>.hidden_size, <span class="hljs-variable language_">self</span>.intermediate_size, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.up_proj = nn.Linear(<span class="hljs-variable language_">self</span>.hidden_size, <span class="hljs-variable language_">self</span>.intermediate_size, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.down_proj = nn.Linear(<span class="hljs-variable language_">self</span>.intermediate_size, <span class="hljs-variable language_">self</span>.hidden_size, bias=<span class="hljs-literal">False</span>)<br>        <span class="hljs-variable language_">self</span>.act_fn = ACT2FN[config.hidden_act]<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        down_proj = <span class="hljs-variable language_">self</span>.down_proj(<span class="hljs-variable language_">self</span>.act_fn(<span class="hljs-variable language_">self</span>.gate_proj(x)) * <span class="hljs-variable language_">self</span>.up_proj(x))<br>        <span class="hljs-keyword">return</span> down_proj<br></code></pre></td></tr></table></figure></div>

<ol>
<li>注意到 <code>intermediate_size = 4*hidden_size</code><ul>
<li>查資料才知道MLP是模型容量的主要來源 <del>我原本一直以為把decoder層數scale up就會有容量 我錯了</del></li>
</ul>
</li>
<li>Gated MLP<ul>
<li>選擇性開關資訊流的MLP</li>
<li>Gated MLP &#x3D; token-level、連續版的 MoE</li>
</ul>
</li>
</ol>
<h3 id="RMSNorm"><a href="#RMSNorm" class="headerlink" title="RMSNorm"></a>RMSNorm</h3><p>最後來談談 RMSNorm。</br><br>跟一般 layer norm 的主要不同就是只對 vector 長度正規化，不會對方向正規劃。換句話說不會引入其他 bias。</br></p>
<ul>
<li>Layer Norm: [x-mean(x)]&#x2F;std(x)</li>
<li>RMS Norm: x &#x2F; ||x||</li>
</ul>
<div class="code-container" data-rel="Python"><figure class="iseeu highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">Qwen3RMSNorm</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, hidden_size, eps: <span class="hljs-built_in">float</span> = <span class="hljs-number">1e-6</span></span>) -&gt; <span class="hljs-literal">None</span>:<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-variable language_">self</span>.weight = nn.Parameter(torch.ones(hidden_size))<br>        <span class="hljs-variable language_">self</span>.variance_epsilon = eps<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, hidden_states: torch.Tensor</span>) -&gt; torch.Tensor:<br>        input_dtype = hidden_states.dtype<br>        hidden_states = hidden_states.to(torch.float32)<br>        variance = hidden_states.<span class="hljs-built_in">pow</span>(<span class="hljs-number">2</span>).mean(-<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)<br>        hidden_states = hidden_states * torch.rsqrt(variance + <span class="hljs-variable language_">self</span>.variance_epsilon)<br>        <span class="hljs-keyword">return</span> <span class="hljs-variable language_">self</span>.weight * hidden_states.to(input_dtype)<br></code></pre></td></tr></table></figure></div>

<ul>
<li>注意到這邊會強制改成fp32，因為RMSNorm在低精度下會產生大誤差</li>
<li>在現代加速引擎中這引入了一些有趣的問題：<a class="link"   target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/10692559975" >RMSNorm的精度陷阱：记一次LLM推理精度调查<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></li>
</ul>
<h2 id="Conlusion"><a href="#Conlusion" class="headerlink" title="Conlusion"></a>Conlusion</h2><p>原本想寫一下 MOE 的，但還是等下一篇好了，之後也會談談Flash Infer等 inference kernel，最後會去看 Sglang vLLM。可能也會看看 LoRA 等一些 MLsys 技巧。</p>

		</div>

		

		
		<ul class="post-tags-box text-lg mt-1.5 flex-wrap justify-center flex md:hidden">
			
			<li class="tag-item mx-0.5">
				<a href="/Blog/tags/Research/">#Research</a>&nbsp;
			</li>
			
			<li class="tag-item mx-0.5">
				<a href="/Blog/tags/MLsys/">#MLsys</a>&nbsp;
			</li>
			
		</ul>
		

		

		
		<div class="article-nav my-8 flex justify-between items-center px-2 sm:px-6 md:px-8">
			
			<div class="article-prev border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="prev" rel="prev" href="/Blog/2026/01/25/MLsys-FlashInfer-Attn/">
					<span class="left arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-left"></i>
					</span>
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item truncate max-w-48">MLsys Note (1) - FlashInfer Cascade Attention &amp; KV Cache Layout</span>
						<span class="post-nav-item">Prev posts</span>
					</span>
				</a>
			</div>
			
			
			<div class="article-next border-border-color shadow-redefine-flat shadow-shadow-color-2 rounded-medium px-4 py-2 hover:shadow-redefine-flat-hover hover:shadow-shadow-color-2">
				<a class="next" rel="next" href="/Blog/2026/01/02/2025-research-and-works/">
					<span class="title flex justify-center items-center">
						<span class="post-nav-title-item truncate max-w-48">2025工作研究流水帳</span>
						<span class="post-nav-item">Next posts</span>
					</span>
					<span class="right arrow-icon flex justify-center items-center">
						<i class="fa-solid fa-chevron-right"></i>
					</span>
				</a>
			</div>
			
		</div>
		


		
		<div class="comment-container px-2 sm:px-6 md:px-8 pb-8">
			<div class="comments-container mt-10 w-full ">
    <div id="comment-anchor" class="w-full h-2.5"></div>
    <div class="comment-area-title w-full my-1.5 md:my-2.5 text-xl md:text-3xl font-bold">
        Comments
    </div>
    

        
            
    <div id="waline"></div>
    <script type="module" data-swup-reload-script>
      import { init } from '/Blog/js/libs/waline.js';

      function loadWaline() {
        init({
          el: '#waline',
          serverURL: 'https://example.example.com',
          dark: 'body[class~="dark-mode"]',
          reaction: false,
          requiredMeta: ['nick', 'mail'],
          emoji: [],
                    lang: 'zh-CN',
        });
      }

      if (typeof swup !== 'undefined') {
        loadWaline();
      } else {
        window.addEventListener('DOMContentLoaded', loadWaline);
      }
    </script>



        
    
</div>

		</div>
		
	</div>

	
	<div class="toc-content-container">
		<div class="post-toc-wrap">
	<div class="post-toc">
		<div class="toc-title">On this page</div>
		<div class="page-title">MLsys Note (0) - Qwen3 Code Walk Through</div>
		<ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Transformers-Code-Walk-Through"><span class="nav-text">Transformers Code Walk Through</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CausalLM"><span class="nav-text">CausalLM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Model"><span class="nav-text">Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Decoder-layers"><span class="nav-text">Decoder layers</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention"><span class="nav-text">Attention</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Rotary-Positional-Embedding-RoPE"><span class="nav-text">Rotary Positional Embedding (RoPE)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#MLP"><span class="nav-text">MLP</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RMSNorm"><span class="nav-text">RMSNorm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Conlusion"><span class="nav-text">Conlusion</span></a></li></ol></li></ol>

	</div>
</div>
	</div>
	
</div>
			</div>

			
		</div>

		<div class="main-content-footer">
			<footer class="footer mt-5 py-5 h-auto text-base text-third-text-color relative border-t-2 border-t-border-color">
    <div class="info-container py-3 text-center">
        
        <div class="text-center">
            &copy;
            
              <span>2022</span>
              -
            
            2026&nbsp;&nbsp;<i class="fa-solid fa-heart fa-beat" style="--fa-animation-duration: 0.5s; color: #f54545"></i>&nbsp;&nbsp;<a href="/Blog/">Kai-Jie Lin</a>
            
                
                <p class="post-count space-x-0.5">
                    <span>
                        24 posts in total
                    </span>
                    
                </p>
            
        </div>
        
            <script data-swup-reload-script src="https://cn.vercount.one/js"></script>
            <div class="relative text-center lg:absolute lg:right-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-right">
                
                    <span id="busuanzi_container_site_uv" class="lg:!block">
                        <span class="text-sm">VISITOR COUNT</span>
                        <span id="busuanzi_value_site_uv"></span>
                    </span>
                
                
                    <span id="busuanzi_container_site_pv" class="lg:!block">
                        <span class="text-sm">TOTAL PAGE VIEWS</span>
                        <span id="busuanzi_value_site_pv"></span>
                    </span>
                
            </div>
        
        <div class="relative text-center lg:absolute lg:left-[20px] lg:top-1/2 lg:-translate-y-1/2 lg:text-left">
            <span class="lg:block text-sm">POWERED BY <?xml version="1.0" encoding="utf-8"?><!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd"><svg class="relative top-[2px] inline-block align-baseline" version="1.1" id="圖層_1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" width="1rem" height="1rem" viewBox="0 0 512 512" enable-background="new 0 0 512 512" xml:space="preserve"><path fill="#0E83CD" d="M256.4,25.8l-200,115.5L56,371.5l199.6,114.7l200-115.5l0.4-230.2L256.4,25.8z M349,354.6l-18.4,10.7l-18.6-11V275H200v79.6l-18.4,10.7l-18.6-11v-197l18.5-10.6l18.5,10.8V237h112v-79.6l18.5-10.6l18.5,10.8V354.6z"/></svg><a target="_blank" class="text-base" href="https://hexo.io">Hexo</a></span>
            <span class="text-sm lg:block">THEME&nbsp;<a class="text-base" target="_blank" href="https://github.com/EvanNotFound/hexo-theme-redefine">Redefine v2.8.5</a></span>
        </div>
        
        
            <div>
                Blog up for <span class="odometer" id="runtime_days" ></span> days <span class="odometer" id="runtime_hours"></span> hrs <span class="odometer" id="runtime_minutes"></span> Min <span class="odometer" id="runtime_seconds"></span> Sec
            </div>
        
        
            <script data-swup-reload-script>
                try {
                    function odometer_init() {
                    const elements = document.querySelectorAll('.odometer');
                    elements.forEach(el => {
                        new Odometer({
                            el,
                            format: '( ddd).dd',
                            duration: 200
                        });
                    });
                    }
                    odometer_init();
                } catch (error) {}
            </script>
        
        
        
    </div>  
</footer>
		</div>
	</div>

	
	<div class="post-tools">
		<div class="post-tools-container">
	<ul class="article-tools-list">
		<!-- TOC aside toggle -->
		
		<li class="right-bottom-tools page-aside-toggle">
			<i class="fa-regular fa-outdent"></i>
		</li>
		

		<!-- go comment -->
		
		<li class="go-comment">
			<i class="fa-regular fa-comments"></i>
		</li>
		
	</ul>
</div>
	</div>
	

	<div class="right-side-tools-container">
		<div class="side-tools-container">
	<ul class="hidden-tools-list">
		<li class="right-bottom-tools tool-font-adjust-plus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-plus"></i>
		</li>

		<li class="right-bottom-tools tool-font-adjust-minus flex justify-center items-center">
			<i class="fa-regular fa-magnifying-glass-minus"></i>
		</li>

		<li class="right-bottom-tools tool-dark-light-toggle flex justify-center items-center">
			<i class="fa-regular fa-moon"></i>
		</li>

		<!-- rss -->
		

		

		<li class="right-bottom-tools tool-scroll-to-bottom flex justify-center items-center">
			<i class="fa-regular fa-arrow-down"></i>
		</li>
	</ul>

	<ul class="visible-tools-list">
		<li class="right-bottom-tools toggle-tools-list flex justify-center items-center">
			<i class="fa-regular fa-cog fa-spin"></i>
		</li>
		
		<li class="right-bottom-tools tool-scroll-to-top flex justify-center items-center">
			<i class="arrow-up fas fa-arrow-up"></i>
			<span class="percent"></span>
		</li>
		
		
	</ul>
</div>
	</div>

	<div class="image-viewer-container">
	<img src="">
</div>

	

</main>



<script src="/Blog/js/build/libs/Swup.min.js"></script>

<script src="/Blog/js/build/libs/SwupSlideTheme.min.js"></script>

<script src="/Blog/js/build/libs/SwupScriptsPlugin.min.js"></script>

<script src="/Blog/js/build/libs/SwupProgressPlugin.min.js"></script>

<script src="/Blog/js/build/libs/SwupScrollPlugin.min.js"></script>

<script src="/Blog/js/build/libs/SwupPreloadPlugin.min.js"></script>

<script>
    const swup = new Swup({
        plugins: [
            new SwupScriptsPlugin({
                optin: true,
            }),
            new SwupProgressPlugin(),
            new SwupScrollPlugin({
                offset: 80,
            }),
            new SwupSlideTheme({
                mainElement: ".main-content-body",
            }),
            new SwupPreloadPlugin(),
        ],
        containers: ["#swup"],
    });
</script>




	
<script src="/Blog/js/build/tools/imageViewer.js" type="module"></script>

<script src="/Blog/js/build/utils.js" type="module"></script>

<script src="/Blog/js/build/main.js" type="module"></script>

<script src="/Blog/js/build/layouts/navbarShrink.js" type="module"></script>

<script src="/Blog/js/build/tools/scrollTopBottom.js" type="module"></script>

<script src="/Blog/js/build/tools/lightDarkSwitch.js" type="module"></script>

<script src="/Blog/js/build/layouts/categoryList.js" type="module"></script>





    
<script src="/Blog/js/build/tools/codeBlock.js" type="module"></script>




    
<script src="/Blog/js/build/layouts/lazyload.js" type="module"></script>




    
<script src="/Blog/js/build/tools/runtime.js"></script>

    
<script src="/Blog/js/build/libs/odometer.min.js"></script>

    
<link rel="stylesheet" href="/Blog/assets/odometer-theme-minimal.css">




  
<script src="/Blog/js/build/libs/Typed.min.js"></script>

  
<script src="/Blog/js/build/plugins/typed.js" type="module"></script>








    
<script src="/Blog/js/build/libs/anime.min.js"></script>





    
<script src="/Blog/js/build/tools/tocToggle.js" type="module" data-swup-reload-script=""></script>

<script src="/Blog/js/build/layouts/toc.js" type="module" data-swup-reload-script=""></script>

<script src="/Blog/js/build/plugins/tabs.js" type="module" data-swup-reload-script=""></script>




<script src="/Blog/js/build/libs/moment-with-locales.min.js" data-swup-reload-script=""></script>


<script src="/Blog/js/build/layouts/essays.js" type="module" data-swup-reload-script=""></script>





	
</body>

</html>